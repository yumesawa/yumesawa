{"meta":{"title":"YUME","subtitle":null,"description":null,"author":"梦沢","url":"http://yoursite.com","root":"/"},"pages":[{"title":"关于","date":"2019-11-14T12:35:55.560Z","updated":"2019-11-14T08:44:59.430Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2019-11-14T12:14:11.836Z","updated":"2019-11-14T12:14:11.835Z","comments":true,"path":"blog/categories/index.html","permalink":"http://yoursite.com/blog/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-11-14T10:51:19.739Z","updated":"2019-11-14T10:51:19.739Z","comments":true,"path":"blog/tags/index.html","permalink":"http://yoursite.com/blog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"读写锁","slug":"读写锁","date":"2019-11-14T12:36:09.321Z","updated":"2019-11-06T00:08:32.290Z","comments":true,"path":"2019/11/14/读写锁/","link":"","permalink":"http://yoursite.com/2019/11/14/%E8%AF%BB%E5%86%99%E9%94%81/","excerpt":"","text":"读写锁若只是单纯的从邮箱中读取数据，则不会产生什么问题，若是在读取的过程中有一个用户试图去删除编号为25的邮件，这时可能读取邮件的用户会报错退出，也可能读到不准确的数据。解决这类问题的方法就是使用读写锁。其中读锁也叫共享锁，写锁叫排它锁。本节呢先不讨论具体的实现方式，只是简单介绍下读写锁的概念。 读锁（共享锁）读锁又称为共享锁，相互之间不堵塞。多个请求可以在同一个资源目标进行读取，互相不干扰。 写锁（排它锁）写锁会阻塞其他的写锁和读锁，这样的话可以保证在同一时间里只有同一个用户能执行写入的操作，防止其他用户读取正在写入的同一资源。在世纪的数据库系统中，每时每刻都在发生锁定，当某个用户在修改某一部分数据时，MySQL会通过锁定防止其他用户读取同一数据。大多数时候，MySQL锁的内部管理都是透明的。 悲观锁（Pessimistic Lock）当我们要对一个数据库中的一条数据进行修改的时候，为了避免同时被其他人修改，最好的办法就是直接对该数据进行加锁以防止并发。这种借助数据库锁机制，在修改数据之前先锁定，再修改的方式被称之为悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）。 百度百科： 悲观锁，正如其名，具有强烈的独占和排他特性。它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度。因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 之所以叫做悲观锁，是因为这是一种对数据的修改抱有悲观态度的并发控制方式。我们一般认为数据被并发修改的概率比较大，所以需要在修改之前先加锁。 悲观锁主要是共享锁或排他锁 共享锁又称为读锁，简称S锁。顾名思义，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。 排他锁又称为写锁，简称X锁。顾名思义，排他锁就是不能与其他锁并存，如果一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据行读取和修改。 悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。 悲观锁 但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会。另外还会降低并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数据。 悲观锁的实现，往往依靠数据库提供的锁机制。在数据库中，悲观锁的流程如下： 在对记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 期间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 乐观锁（ Optimistic Locking ）乐观锁是相对悲观锁而言的，乐观锁假设数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回给用户错误的信息，让用户决定如何去做。 百度百科： 乐观锁机制采取了更加宽松的加锁机制。乐观锁是相对悲观锁而言，也是为了避免数据库幻读、业务处理时间过长等原因引起数据处理错误的一种机制，但乐观锁不会刻意使用数据库本身的锁机制，而是依据数据本身来保证数据的正确性。 相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 乐观锁 乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。 使用乐观锁就不需要借助数据库的锁机制了。 乐观锁的概念中其实已经阐述了它的具体实现细节。主要就是两个步骤：冲突检测和数据更新。其实现方式有一种比较典型的就是CAS(Compare and Swap)。 CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。","categories":[],"tags":[]},{"title":"python读写、创建文件","slug":"python读写、创建文件","date":"2019-11-14T12:36:09.319Z","updated":"2019-11-06T02:00:32.525Z","comments":true,"path":"2019/11/14/python读写、创建文件/","link":"","permalink":"http://yoursite.com/2019/11/14/python%E8%AF%BB%E5%86%99%E3%80%81%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6/","excerpt":"","text":"python 读写、创建 文件 python中对文件、文件夹（文件操作函数）的操作需要涉及到os模块和shutil模块。 得到当前工作目录，即当前Python脚本工作的目录路径: os.getcwd() 返回指定目录下的所有文件和目录名:os.listdir() 函数用来删除一个文件:os.remove() 删除多个目录：os.removedirs ** **（r“c：\\python”） 检验给出的路径是否是一个文件：os.path.isfile() 检验给出的路径是否是一个目录：os.path.isdir() 判断是否是绝对路径：os.path.isabs() 检验给出的路径是否真地存:os.path.exists() 返回一个路径的目录名和文件名:os.path.split() eg os.path.split(‘/home/swaroop/byte/code/poem.txt’) 结果：(‘/home/swaroop/byte/code’, ‘poem.txt’) 分离扩展名：os.path.splitext() 获取路径名：os.path.dirname() 获取文件名：os.path.basename() 运行shell命令: os.system() 读取和设置环境变量:os.getenv()与os.putenv() 给出当前平台使用的行终止符:os.linesep Windows使用’\\r\\n’，Linux使用’\\n’而Mac使用’\\r’ 指示你正在使用的平台：os.name 对于Windows，它是’nt’，而对于Linux/Unix用户，它是’posix’ 重命名：os.rename （old，new） 创建多级目录：os.makedirs （r“c：\\python\\test”） 创建单个目录：os.mkdir （“test”） 获取文件属性：os.stat （file） 修改文件权限与时间戳：os.chmod （file） 终止当前进程：os.exit （） 获取文件大小：os.path.getsize （filename） 文件操作： os.mknod(“test.txt”) 创建空文件 fp = open(“test.txt”,w) 直接打开一个文件，如果文件不存在则创建文件 关于open 模式： w 以写方式打开， a 以追加模式打开 (从 EOF 开始, 必要时创建新文件) r+ 以读写模式打开 w+ 以读写模式打开 (参见 w ) a+ 以读写模式打开 (参见 a ) rb 以二进制读模式打开 wb 以二进制写模式打开 (参见 w ) ab 以二进制追加模式打开 (参见 a ) rb+ 以二进制读写模式打开 (参见 r+ ) wb+ 以二进制读写模式打开 (参见 w+ ) ab+ 以二进制读写模式打开 (参见 a+ ) fp.read([size]) #size为读取的长度，以byte为单位 fp.readline([size]) #读一行，如果定义了size，有可能返回的只是一行的一部分 fp.readlines([size]) #把文件每一行作为一个list的一个成员，并返回这个list。其实它的内部是通过循环调用readline()来实现的。如果提供size参数，size是表示读取内容的总长，也就是说可能只读到文件的一部分。 fp.write(str) #把str写到文件中，write()并不会在str后加上一个换行符 fp.writelines(seq) #把seq的内容全部写到文件中(多行一次性写入)。这个函数也只是忠实地写入，不会在每行后面加上任何东西。 fp.close() #关闭文件。python会在一个文件不用后自动关闭文件，不过这一功能没有保证，最好还是养成自己关闭的习惯。 如果一个文件在关闭后还对其进行操作会产生ValueError fp.flush() #把缓冲区的内容写入硬盘 fp.fileno() #返回一个长整型的”文件标签“ fp.isatty() #文件是否是一个终端设备文件（unix系统中的） fp.tell() #返回文件操作标记的当前位置，以文件的开头为原点 fp.next() #返回下一行，并将文件操作标记位移到下一行。把一个file用于for … in file这样的语句时，就是调用next()函数来实现遍历的。 fp.seek(offset[,whence]) #将文件打操作标记移到offset的位置。这个offset一般是相对于文件的开头来计算的，一般为正数。但如果提供了whence参数就不一定了，whence可以为0表示从头开始计算，1表示以当前位置为原点计算。2表示以文件末尾为原点进行计算。需要注意，如果文件以a或a+的模式打开，每次进行写操作时，文件操作标记会自动返回到文件末尾。 fp.truncate([size]) #把文件裁成规定的大小，默认的是裁到当前文件操作标记的位置。如果size比文件的大小还要大，依据系统的不同可能是不改变文件，也可能是用0把文件补到相应的大小，也可能是以一些随机的内容加上去。 目录操作： os.mkdir(“file”) 创建目录 复制文件： shutil.copyfile(“oldfile”,”newfile”) oldfile和newfile都只能是文件 shutil.copy(“oldfile”,”newfile”) oldfile只能是文件夹，newfile可以是文件，也可以是目标目录 复制文件夹： shutil.copytree(“olddir”,”newdir”) olddir和newdir都只能是目录，且newdir必须不存在 重命名文件（目录） os.rename(“oldname”,”newname”) 文件或目录都是使用这条命令 移动文件（目录） shutil.move(“oldpos”,”newpos”) 删除文件 os.remove(“file”) 删除目录 os.rmdir(“dir”)只能删除空目录 shutil.rmtree(“dir”) 空目录、有内容的目录都可以删 转换目录 os.chdir(“path”) 换路径 Python读写文件 1.open 使用open打开文件后一定要记得调用文件对象的close()方法。比如可以用try/finally语句来确保最后能关闭文件。 file_object = open(‘thefile.txt’) try: all_the_text = file_object.read( ) finally: file_object.close( ) 注：不能把open语句放在try块里，因为当打开文件出现异常时，文件对象file_object无法执行close()方法。 2.读文件 读文本文件 input = open(‘data’, ‘r’) #第二个参数默认为r input = open(‘data’) 读二进制文件 input = open(‘data’, ‘rb’) 读取所有内容 file_object = open(‘thefile.txt’) try: all_the_text = file_object.read( ) finally: file_object.close( ) 读固定字节 file_object = open(‘abinfile’, ‘rb’) try: while True: chunk = file_object.read(100) if not chunk: break do_something_with(chunk) finally: file_object.close( ) 读每行 list_of_all_the_lines = file_object.readlines( ) 如果文件是文本文件，还可以直接遍历文件对象获取每行： for line in file_object: process line 3.写文件 写文本文件 output = open(‘data’, ‘w’) 写二进制文件 output = open(‘data’, ‘wb’) 追加写文件 output = open(‘data’, ‘w+’) 写数据 file_object = open(‘thefile.txt’, ‘w’) file_object.write(all_the_text) file_object.close( ) 写入多行 file_object.writelines(list_of_text_strings) 注意，调用writelines写入多行在性能上会比使用write一次性写入要高。 在处理日志文件的时候，常常会遇到这样的情况：日志文件巨大，不可能一次性把整个文件读入到内存中进行处理，例如需要在一台物理内存为 2GB 的机器上处理一个 2GB 的日志文件，我们可能希望每次只处理其中 200MB 的内容。 在 Python 中，内置的 File 对象直接提供了一个 readlines(sizehint) 函数来完成这样的事情。以下面的代码为例： file = open(‘test.log’, ‘r’)sizehint = 209715200 # 200Mposition = 0lines = file.readlines(sizehint)while not file.tell() - position &lt; 0: position = file.tell() lines = file.readlines(sizehint) 每次调用 readlines(sizehint) 函数，会返回大约 200MB 的数据，而且所返回的必然都是完整的行数据，大多数情况下，返回的数据的字节数会稍微比 sizehint 指定的值大一点（除最后一次调用 readlines(sizehint) 函数的时候）。通常情况下，Python 会自动将用户指定的 sizehint 的值调整成内部缓存大小的整数倍。 file在python是一个特殊的类型，它用于在python程序中对外部的文件进行操作。在python中一切都是对象，file也不例外，file有file的方法和属性。下面先来看如何创建一个file对象： file(name[, mode[, buffering]]) file()函数用于创建一个file对象，它有一个别名叫open()，可能更形象一些，它们是内置函数。来看看它的参数。它参数都是以字符串的形式传递的。name是文件的名字。 mode是打开的模式，可选的值为r w a U，分别代表读（默认） 写 添加支持各种换行符的模式。用w或a模式打开文件的话，如果文件不存在，那么就自动创建。此外，用w模式打开一个已经存在的文件时，原有文件的内容会被清空，因为一开始文件的操作的标记是在文件的开头的，这时候进行写操作，无疑会把原有的内容给抹掉。由于历史的原因，换行符在不同的系统中有不同模式，比如在 unix中是一个\\n，而在windows中是‘\\r\\n’，用U模式打开文件，就是支持所有的换行模式，也就说‘\\r’ ‘\\n’ ‘\\r\\n’都可表示换行，会有一个tuple用来存贮这个文件中用到过的换行符。不过，虽说换行有多种模式，读到python中统一用\\n代替。在模式字符的后面，还可以加上+ b t这两种标识，分别表示可以对文件同时进行读写操作和用二进制模式、文本模式（默认）打开文件。 buffering如果为0表示不进行缓冲;如果为1表示进行“行缓冲“;如果是一个大于1的数表示缓冲区的大小，应该是以字节为单位的。 file对象有自己的属性和方法。先来看看file的属性。 closed #标记文件是否已经关闭，由close()改写 encoding #文件编码 mode #打开模式 name #文件名 newlines #文件中用到的换行模式，是一个tuple softspace #boolean型，一般为0，据说用于print file的读写方法： F.close() #关闭文件。 python会在一个文件不用后自动关闭文件，不过这一功能没有保证，最好还是养成自己关闭的习惯。如果一个文件在关闭后还对其进行操作会产生ValueError 。 F.flush() #把缓冲区的内容写入硬盘 F.fileno() #返回一个长整型的”文件标签“ F.isatty() #文件是否是一个终端设备文件（unix系统中的） F.tell() #返回文件操作标记的当前位置，以文件的开头为原点 F.next() #返回下一行，并将文件操作标记位移到下一行。把一个file用于for … in file这样的语句时，就是调用next()函数来实现遍历的。 F.seek(offset[,whence]) #将文件打操作标记移到offset的位置。这个offset一般是相对于文件的开头来计算的，一般为正数。但如果提供了whence参数就不一定了，whence可以为0表示从头开始计算，1表示以当前位置为原点计算。2表示以文件末尾为原点进行计算。需要注意，如果文件以a或a+的模式打开，每次进行写操作时，文件操作标记会自动返回到文件末尾。 F.truncate([size]) #把文件裁成规定的大小，默认的是裁到当前文件操作标记的位置。如果size比文件的大小还要大，依据系统的不同可能是不改变文件，也可能是用0把文件补到相应的大小，也可能是以一些随机的内容加上去。","categories":[],"tags":[]},{"title":"mysql要看的理论","slug":"mysql要看的理论","date":"2019-11-14T12:36:09.318Z","updated":"2019-10-28T12:38:24.295Z","comments":true,"path":"2019/11/14/mysql要看的理论/","link":"","permalink":"http://yoursite.com/2019/11/14/mysql%E8%A6%81%E7%9C%8B%E7%9A%84%E7%90%86%E8%AE%BA/","excerpt":"","text":"MySQL的并发控制多个查询需要在同一时刻修改数据，会产生并发控制问题。例如：两个进程在同一时刻对同一个邮箱进行投递邮件，会发生什么情况呢？显然，邮箱的数据会被损坏，两封邮件的内容会交叉地附加在邮箱文件的末尾。设计良好的邮箱投递系统会通过锁来防止数据损坏。但这并不支持并发处理。因为人以时刻只有一个进程可以修改邮箱数据。 并发控制这里主要介绍两个概念，一个是读写锁，一个是锁的粒度。 读写锁若只是单纯的从邮箱中读取数据，则不会产生什么问题，若是在读取的过程中有一个用户试图去删除编号为25的邮件，这时可能读取邮件的用户会报错退出，也可能读到不准确的数据。解决这类问题的方法就是使用读写锁。其中读锁也叫共享锁，写锁叫排它锁。本节呢先不讨论具体的实现方式，只是简单介绍下读写锁的概念。 读锁（共享锁）读锁又称为共享锁，相互之间不堵塞。多个请求可以在同一个资源目标进行读取，互相不干扰。 写锁（排它锁）写锁会阻塞其他的写锁和读锁，这样的话可以保证在同一时间里只有同一个用户能执行写入的操作，防止其他用户读取正在写入的同一资源。在世纪的数据库系统中，每时每刻都在发生锁定，当某个用户在修改某一部分数据时，MySQL会通过锁定防止其他用户读取同一数据。大多数时候，MySQL锁的内部管理都是透明的。 锁粒度锁粒度让提高共享资源的并发性更有选择性。尽量只锁定需要修改的部分数据。理想的方式就是只会对修改的数据片进行精确的锁定。这里不是说锁定的数量越少，则系统的并发程度越高，只要相互之间不发生冲突即可，还要考虑加锁消耗的资源问题（获得锁、检查锁是否已经解除、释放锁等），因为加锁的同时也会增加系统的开销，如果是花费大量时间来管理锁，而不是存取数据，那么系统的性能可能会因此受到影响。 表锁MySQL独立于存储引擎提供表锁，例如，对于ALTER TABLE语句，服务器提供表锁(table-level lock)。 表锁是MySQL中最基本的锁策略，并且是开销最小的策略。当一个表要插入、删除、更新记录时，首先需要获得写锁，这会阻塞其他用户对该表的读写操作。只有没有写锁的时候，其他用户才能进行读取的操作。读锁之间是不进行相互阻塞的。（写锁优先级高于读锁，写锁的请求可能会被插入到读锁队列的前面） 行级锁InnoDB和Falcon存储引擎提供行级锁，此外，BDB支持页级锁。 行级锁可以最大程度地支持并发处理，同时也带来了最大的锁开销问题。行级锁只在存储引擎中进行实现，服务层完全不了解存储引擎中的实现。 事务MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 一般来说，事务是必须满足4个条件（ACID）：：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。 原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 二、事务的并发问题https://blog.csdn.net/w_linux/article/details/79666086 *1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 * 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 三、MySQL事务隔离级别 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 MySQL的事务隔离级别 1、读未提交：能读到未提交的数据。 2、读已提交：读已提交的数据。 3、可重复读：mysql默认，查询的都是事务开始时的数据。 4、串行读：完全串行化读，每次都会锁表，读写互相阻塞。 MySQL的事务隔离级别1、读未提交：能读到未提交的数据。 2、读已提交：读已提交的数据。 3、可重复读：mysql默认，查询的都是事务开始时的数据。 4、串行读：完全串行化读，每次都会锁表，读写互相阻塞。 死锁产生 所谓死锁：是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用，它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环。 ​ 当事务试图以不同的顺序锁定资源时，就可能产生死锁。 多个事务同时锁定同一个资源时也可能会产生死锁。 ​ 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。 检测死锁数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误。 InnoDB目前处理死锁的方法是,将持有最少行级排他锁的事务进行回滚(这是相对比较简单的死锁回滚算法)。 事务日志事务日志可以帮助提高事务的效率。使用事务日志,存储引擎在修改表的数据时只需要修改其内存拷贝,再把该修改行为记录到持久在硬盘上的事务日志中,而不用每次都将修改的数据本身持久到磁盘。 事务日志采用的是追加的方式,因此写日志的操作是磁盘上一小块区域内的顺序I/O,1而不像随机I/O需要在磁盘的多个地方移动磁头,所以采用 “事务日志的方式相对来说要快得多。事多日志持久以后,内存中被修改的数据在后台可以慢慢地刷回到磁盘。目前大多数存储引擎都是这样实现的,我们通常称之为预写式日志(Write-Ahead Logging),修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化,但数据本身还没有写回磁盘,此时系统崩溃,存储引擎在重启时能够自动恢复这部分修改的数据。具体的恢复t式则视存储引擎而定。 说到事务日志，不得不说的就是redo和undo。 1.redo log 在innoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志先行”(Write-Ahead Logging)。当事务提交之后，在Buffer Pool中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态。未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定。 1234567891011在系统启动的时候，就已经为redo log分配了一块连续的存储空间,以顺序追加的方式记录Redo Log,通过顺序IO来改善性能。所有的事务共享redo log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起。如下一个简单示例：记录1：&lt;trx1, insert...&gt;记录2：&lt;trx2, delete...&gt;记录3：&lt;trx3, update...&gt;记录4：&lt;trx1, update...&gt;记录5：&lt;trx3, insert...&gt; 2.undo log undo log主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。单个事务的回滚，只会回滚当前事务做的操作，并不会影响到其他的事务做的操作。 1234567891011121314151617181920212223以下是undo+redo事务的简化过程假设有2个数值，分别为A和B,值为1，21. start transaction;2. 记录 A=1 到undo log;3. update A = 3；4. 记录 A=3 到redo log；5. 记录 B=2 到undo log；6. update B = 4；7. 记录B = 4 到redo log；8. 将redo log刷新到磁盘9. commit在1-8的任意一步系统宕机，事务未提交，该事务就不会对磁盘上的数据做任何影响。如果在8-9之间宕机，恢复之后可以选择回滚，也可以选择继续完成事务提交，因为此时redo log已经持久化。若在9之后系统宕机，内存映射中变更的数据还来不及刷回磁盘，那么系统恢复之后，可以根据redo log把数据刷回磁盘。 MySQL中的事务MysQL提供了两种事务型的存储引擎: InnoDB和NDB Cluster, 另外还有一些第三方存储引擎也支持事务,比较知名的包括XtraDB和PBXT. 自动提交(AUTOCOMMIT) 在事务中混合使用存储引擎 隐式和显式锁定 MySQL中多版本并发控制(MVCC)简介MVCC（Multi-Version Concurrency Control）即多版本并发控制。 MySQL的大多数事务型（如InnoDB,Falcon等）存储引擎实现的都不是简单的行级锁。基于提升并发性能的考虑，他们一般都同时实现了MVCC。当前不仅仅是MySQL,其它数据库系统（如Oracle,PostgreSQL）也都实现了MVCC。值得注意的是MVCC并没有一个统一的实现标准，所以不同的数据库，不同的存储引擎的实现都不尽相同。 MVCC优缺点MVCC在大多数情况下代替了行锁，实现了对读的非阻塞，读不加锁，读写不冲突。缺点是每行记录都需要额外的存储空间，需要做更多的行维护和检查工作。 MVCC的实现原理undo log为了便于理解MVCC的实现原理，这里简单介绍一下undo log的工作过程 在不考虑redo log 的情况下利用undo log工作的简化过程为： 序号 动作 1 开始事务 2 记录数据行数据快照到undo log 3 更新数据 4 将undo log写到磁盘 5 将数据写到磁盘 6 提交事务 1）为了保证数据的持久性数据要在事务提交之前持久化 2）undo log的持久化必须在在数据持久化之前，这样才能保证系统崩溃时，可以用undo log来回滚事务 Innodb中的隐藏列Innodb通过undo log保存了已更改行的旧版本的信息的快照。 InnoDB的内部实现中为每一行数据增加了三个隐藏列用于实现MVCC。 列名 长度(字节) 作用 DB_TRX_ID 6 插入或更新行的最后一个事务的事务标识符。（删除视为更新，将其标记为已删除） DB_ROLL_PTR 7 写入回滚段的撤消日志记录（若行已更新，则撤消日志记录包含在更新行之前重建行内容所需的信息） DB_ROW_ID 6 行标识（隐藏单调自增id） 结构 数据列 .. DB_ROW_ID DB_TRX_ID DB_ROLL_PTR MVCC工作过程 MVCC只在READ COMMITED 和 REPEATABLE READ 两个隔离级别下工作。READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行。而SERIALIZABLE 则会对所有读取的行都加锁 SELECTInnoDB 会根据两个条件来检查每行记录： InnoDB只查找版本(DB_TRX_ID)早于当前事务版本的数据行（行的系统版本号&lt;=事务的系统版本号,这样可以确保数据行要么是在开始之前已经存在了，要么是事务自身插入或修改过的） 行的删除版本号(DB_ROLL_PTR)要么未定义（未更新过），要么大于当前事务版本号（在当前事务开始之后更新的）。这样可以确保事务读取到的行，在事务开始之前未被删除。 INSERTInnoDB为新插入的每一行保存当前系统版本号作为行版本号 DELETEInnoDB为删除的每一行保存当前的系统版本号作为行删除标识 UPDATEInnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识","categories":[],"tags":[]},{"title":"MySQL的并发控制","slug":"MySQL的并发控制","date":"2019-11-14T12:36:09.316Z","updated":"2019-10-28T23:54:14.708Z","comments":true,"path":"2019/11/14/MySQL的并发控制/","link":"","permalink":"http://yoursite.com/2019/11/14/MySQL%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/","excerpt":"","text":"import timeDEBUG = 0 # 在需要分析时效性的时候将该量置为1，否则置为0 def print_func_time(function): “”” 计算程序运行时间 :param function: :return: “”” def f(args, *kwargs): if DEBUG: t0 = time.time() result = function(args, *kwargs) t1 = time.time() spend = t1 - t0 print(“运行耗时%.3f 秒：函数%s” % (spend, function.name)) else: result = function(args, *kwargs) return result return f @print_func_timedef test(): print(123) if name == ‘main‘: t00 = time.time() test() t10 = time.time() spend1 = t10 - t00 print(“运行耗时%.7f秒” % spend1) MySQL的并发控制多个查询需要在同一时刻修改数据，会产生并发控制问题。例如：两个进程在同一时刻对同一个邮箱进行投递邮件，会发生什么情况呢？显然，邮箱的数据会被损坏，两封邮件的内容会交叉地附加在邮箱文件的末尾。设计良好的邮箱投递系统会通过锁来防止数据损坏。但这并不支持并发处理。因为人以时刻只有一个进程可以修改邮箱数据。 并发控制这里主要介绍两个概念，一个是读写锁，一个是锁的粒度。 读写锁若只是单纯的从邮箱中读取数据，则不会产生什么问题，若是在读取的过程中有一个用户试图去删除编号为25的邮件，这时可能读取邮件的用户会报错退出，也可能读到不准确的数据。解决这类问题的方法就是使用读写锁。其中读锁也叫共享锁，写锁叫排它锁。本节呢先不讨论具体的实现方式，只是简单介绍下读写锁的概念。 读锁（共享锁）读锁又称为共享锁，相互之间不堵塞。多个请求可以在同一个资源目标进行读取，互相不干扰。 写锁（排它锁）写锁会阻塞其他的写锁和读锁，这样的话可以保证在同一时间里只有同一个用户能执行写入的操作，防止其他用户读取正在写入的同一资源。在世纪的数据库系统中，每时每刻都在发生锁定，当某个用户在修改某一部分数据时，MySQL会通过锁定防止其他用户读取同一数据。大多数时候，MySQL锁的内部管理都是透明的。 锁粒度锁粒度让提高共享资源的并发性更有选择性。尽量只锁定需要修改的部分数据。理想的方式就是只会对修改的数据片进行精确的锁定。这里不是说锁定的数量越少，则系统的并发程度越高，只要相互之间不发生冲突即可，还要考虑加锁消耗的资源问题（获得锁、检查锁是否已经解除、释放锁等），因为加锁的同时也会增加系统的开销，如果是花费大量时间来管理锁，而不是存取数据，那么系统的性能可能会因此受到影响。 表锁MySQL独立于存储引擎提供表锁，例如，对于ALTER TABLE语句，服务器提供表锁(table-level lock)。 表锁是MySQL中最基本的锁策略，并且是开销最小的策略。当一个表要插入、删除、更新记录时，首先需要获得写锁，这会阻塞其他用户对该表的读写操作。只有没有写锁的时候，其他用户才能进行读取的操作。读锁之间是不进行相互阻塞的。（写锁优先级高于读锁，写锁的请求可能会被插入到读锁队列的前面） 行级锁InnoDB和Falcon存储引擎提供行级锁，此外，BDB支持页级锁。 行级锁可以最大程度地支持并发处理，同时也带来了最大的锁开销问题。行级锁只在存储引擎中进行实现，服务层完全不了解存储引擎中的实现。 事务MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 一般来说，事务是必须满足4个条件（ACID）：：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。 原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 二、事务的并发问题https://blog.csdn.net/w_linux/article/details/79666086 *1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 * 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 三、MySQL事务隔离级别 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 MySQL的事务隔离级别 1、读未提交：能读到未提交的数据。 2、读已提交：读已提交的数据。 3、可重复读：mysql默认，查询的都是事务开始时的数据。 4、串行读：完全串行化读，每次都会锁表，读写互相阻塞。 MySQL的事务隔离级别1、读未提交：能读到未提交的数据。 2、读已提交：读已提交的数据。 3、可重复读：mysql默认，查询的都是事务开始时的数据。 4、串行读：完全串行化读，每次都会锁表，读写互相阻塞。 死锁产生 所谓死锁：是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用，它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环。 ​ 当事务试图以不同的顺序锁定资源时，就可能产生死锁。 多个事务同时锁定同一个资源时也可能会产生死锁。 ​ 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。 检测死锁数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误。 InnoDB目前处理死锁的方法是,将持有最少行级排他锁的事务进行回滚(这是相对比较简单的死锁回滚算法)。 事务日志事务日志可以帮助提高事务的效率。使用事务日志,存储引擎在修改表的数据时只需要修改其内存拷贝,再把该修改行为记录到持久在硬盘上的事务日志中,而不用每次都将修改的数据本身持久到磁盘。 事务日志采用的是追加的方式,因此写日志的操作是磁盘上一小块区域内的顺序I/O,1而不像随机I/O需要在磁盘的多个地方移动磁头,所以采用 “事务日志的方式相对来说要快得多。事多日志持久以后,内存中被修改的数据在后台可以慢慢地刷回到磁盘。目前大多数存储引擎都是这样实现的,我们通常称之为预写式日志(Write-Ahead Logging),修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化,但数据本身还没有写回磁盘,此时系统崩溃,存储引擎在重启时能够自动恢复这部分修改的数据。具体的恢复t式则视存储引擎而定。 说到事务日志，不得不说的就是redo和undo。 1.redo log 在innoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志先行”(Write-Ahead Logging)。当事务提交之后，在Buffer Pool中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态。未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定。 1234567891011在系统启动的时候，就已经为redo log分配了一块连续的存储空间,以顺序追加的方式记录Redo Log,通过顺序IO来改善性能。所有的事务共享redo log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起。如下一个简单示例：记录1：&lt;trx1, insert...&gt;记录2：&lt;trx2, delete...&gt;记录3：&lt;trx3, update...&gt;记录4：&lt;trx1, update...&gt;记录5：&lt;trx3, insert...&gt; 2.undo log undo log主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。单个事务的回滚，只会回滚当前事务做的操作，并不会影响到其他的事务做的操作。 1234567891011121314151617181920212223以下是undo+redo事务的简化过程假设有2个数值，分别为A和B,值为1，21. start transaction;2. 记录 A=1 到undo log;3. update A = 3；4. 记录 A=3 到redo log；5. 记录 B=2 到undo log；6. update B = 4；7. 记录B = 4 到redo log；8. 将redo log刷新到磁盘9. commit在1-8的任意一步系统宕机，事务未提交，该事务就不会对磁盘上的数据做任何影响。如果在8-9之间宕机，恢复之后可以选择回滚，也可以选择继续完成事务提交，因为此时redo log已经持久化。若在9之后系统宕机，内存映射中变更的数据还来不及刷回磁盘，那么系统恢复之后，可以根据redo log把数据刷回磁盘。 MySQL中的事务MysQL提供了两种事务型的存储引擎: InnoDB和NDB Cluster, 另外还有一些第三方存储引擎也支持事务,比较知名的包括XtraDB和PBXT. 自动提交(AUTOCOMMIT) 在事务中混合使用存储引擎 隐式和显式锁定 MySQL中多版本并发控制(MVCC)简介MVCC（Multi-Version Concurrency Control）即多版本并发控制。 MySQL的大多数事务型（如InnoDB,Falcon等）存储引擎实现的都不是简单的行级锁。基于提升并发性能的考虑，他们一般都同时实现了MVCC。当前不仅仅是MySQL,其它数据库系统（如Oracle,PostgreSQL）也都实现了MVCC。值得注意的是MVCC并没有一个统一的实现标准，所以不同的数据库，不同的存储引擎的实现都不尽相同。 MVCC优缺点MVCC在大多数情况下代替了行锁，实现了对读的非阻塞，读不加锁，读写不冲突。缺点是每行记录都需要额外的存储空间，需要做更多的行维护和检查工作。 MVCC的实现原理undo log为了便于理解MVCC的实现原理，这里简单介绍一下undo log的工作过程 在不考虑redo log 的情况下利用undo log工作的简化过程为： 序号 动作 1 开始事务 2 记录数据行数据快照到undo log 3 更新数据 4 将undo log写到磁盘 5 将数据写到磁盘 6 提交事务 1）为了保证数据的持久性数据要在事务提交之前持久化 2）undo log的持久化必须在在数据持久化之前，这样才能保证系统崩溃时，可以用undo log来回滚事务 Innodb中的隐藏列Innodb通过undo log保存了已更改行的旧版本的信息的快照。 InnoDB的内部实现中为每一行数据增加了三个隐藏列用于实现MVCC。 列名 长度(字节) 作用 DB_TRX_ID 6 插入或更新行的最后一个事务的事务标识符。（删除视为更新，将其标记为已删除） DB_ROLL_PTR 7 写入回滚段的撤消日志记录（若行已更新，则撤消日志记录包含在更新行之前重建行内容所需的信息） DB_ROW_ID 6 行标识（隐藏单调自增id） 结构 数据列 .. DB_ROW_ID DB_TRX_ID DB_ROLL_PTR MVCC工作过程 MVCC只在READ COMMITED 和 REPEATABLE READ 两个隔离级别下工作。READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行。而SERIALIZABLE 则会对所有读取的行都加锁 SELECTInnoDB 会根据两个条件来检查每行记录： InnoDB只查找版本(DB_TRX_ID)早于当前事务版本的数据行（行的系统版本号&lt;=事务的系统版本号,这样可以确保数据行要么是在开始之前已经存在了，要么是事务自身插入或修改过的） 行的删除版本号(DB_ROLL_PTR)要么未定义（未更新过），要么大于当前事务版本号（在当前事务开始之后更新的）。这样可以确保事务读取到的行，在事务开始之前未被删除。 INSERTInnoDB为新插入的每一行保存当前系统版本号作为行版本号 DELETEInnoDB为删除的每一行保存当前的系统版本号作为行删除标识 UPDATEInnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识","categories":[],"tags":[]},{"title":"MySQL InnoDB事务隔离级别脏读、可重复读、幻读","slug":"MySQL InnoDB事务隔离级别脏读、可重复读、幻读","date":"2019-11-14T12:36:09.315Z","updated":"2019-10-23T23:43:16.246Z","comments":true,"path":"2019/11/14/MySQL InnoDB事务隔离级别脏读、可重复读、幻读/","link":"","permalink":"http://yoursite.com/2019/11/14/MySQL%20InnoDB%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E8%84%8F%E8%AF%BB%E3%80%81%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E3%80%81%E5%B9%BB%E8%AF%BB/","excerpt":"","text":"MySQL InnoDB事务隔离级别脏读、可重复读、幻读MySQL InnoDB事务的隔离级别有四级，默认是“可重复读” 1）.脏读。另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据。 2）.不重复读。解决了脏读后，会遇到，同一个事务执行过程中，另外一个事务提交了新数据，因此本事务先后两次读到的数据结果会不一致。 3）.幻读。解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。但是，如果另一个事务同时提交了新数据，本事务再更新时，就会“惊奇的”发现了这些新数据，貌似之前读到的数据是“鬼影”一样的幻觉。 1). 脏读 脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 首先区分脏页和脏数据 脏页是内存的缓冲池中已经修改的page，未及时flush到硬盘，但已经写到redo log中。读取和修改缓冲池的page很正常，可以提高效率，flush即可同步。脏数据是指事务对缓冲池中的行记录record进行了修改，但是还没提交！！！，如果这时读取缓冲池中未提交的行数据就叫脏读，违反了事务的隔离性。 2). 不可重复读 是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，第二个事务已经提交。那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题 3). 幻读 : 是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。例如，一个编辑人员更改作者提交的文档，但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料添加到该文档中。如果在编辑人员和生产部门完成对原始文档的处理之前，任何人都不能将新材料添加到文档中，则可以避免该问题。MySQL InnoDB事务默认隔离级别是可重复读并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是next-key locks。 隔离级别 脏读 不重复读 幻读 为提交读（RU） 可能 可能 可能 已提交读（RC） 不可能 可能 可能 可重复读（RR） 不可能 不可能 可能 串行化（Ser） 不可能 不可能 不可能","categories":[],"tags":[]},{"title":"markdown语法","slug":"markdown语法","date":"2019-11-14T12:36:09.312Z","updated":"2019-11-14T12:59:43.609Z","comments":true,"path":"2019/11/14/markdown语法/","link":"","permalink":"http://yoursite.com/2019/11/14/markdown%E8%AF%AD%E6%B3%95/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Linux安装python3","slug":"Linux安装python3","date":"2019-11-14T12:36:09.310Z","updated":"2019-10-28T12:37:31.030Z","comments":true,"path":"2019/11/14/Linux安装python3/","link":"","permalink":"http://yoursite.com/2019/11/14/Linux%E5%AE%89%E8%A3%85python3/","excerpt":"","text":"CentOS7下安装python3 1、安装pyhton3.7 的依赖包yum -y groupinstall “Development tools”yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-develreadline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel 2、下载python3.7的“源码”：wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xz 3、解压并编译安装：tar -xJvf Python-3.7.0.tar.xz 4、用cd命令进入解压出来的Python文件夹cd Python-3.7.0 5、用./方法执行configure,并指定安装到usr目录下./configure –prefix=/usr/local/python3 –enable-shared 6、开始编译安装make &amp;&amp; make install 7、配置环境变量， 创建软链接ln -s /usr/local/python3/bin/python3 /usr/bin/python3 # 创建python3的软链接ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 # 创建pip的软链接 8、将编译目录下的libpython3.7m.so.1.0文件复制到cp /root/Python-3.7.0/libpython3.7m.so.1.0 /usr/lib64/libpython3.7m.so.1.0","categories":[],"tags":[]},{"title":"InnoDB存储引擎","slug":"InnoDB存储引擎","date":"2019-11-14T12:36:09.308Z","updated":"2019-11-04T23:55:35.993Z","comments":true,"path":"2019/11/14/InnoDB存储引擎/","link":"","permalink":"http://yoursite.com/2019/11/14/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/","excerpt":"","text":"InnoDB存储引擎InnoDB是事务型数据库的首选引擎，支持事务安全表（ACID），支持行锁定和外键，上图也看到了，InnoDB是默认的MySQL引擎。InnoDB主要特性有：1、InnoDB给MySQL提供了具有提交、回滚和崩溃恢复能力的事物安全（ACID兼容）存储引擎。InnoDB锁定在行级并且也在SELECT语句中提供一个类似Oracle的非锁定读。这些功能增加了多用户部署和性能。在SQL查询中，可以自由地将InnoDB类型的表和其他MySQL的表类型混合起来，甚至在同一个查询中也可以混合2、InnoDB是为处理巨大数据量的最大性能设计。它的CPU效率可能是任何其他基于磁盘的关系型数据库引擎锁不能匹敌的3、InnoDB存储引擎完全与MySQL服务器整合，InnoDB存储引擎为在主内存中缓存数据和索引而维持它自己的缓冲池。InnoDB将它的表和索引在一个逻辑表空间中，表空间可以包含数个文件（或原始磁盘文件）。这与MyISAM表不同，比如在MyISAM表中每个表被存放在分离的文件中。InnoDB表可以是任何尺寸，即使在文件尺寸被限制为2GB的操作系统上4、InnoDB支持外键完整性约束，存储表中的数据时，每张表的存储都按主键顺序存放，如果没有显示在表定义时指定主键，InnoDB会为每一行生成一个6字节的ROWID，并以此作为主键5、InnoDB被用在众多需要高性能的大型数据库站点上InnoDB不创建目录，使用InnoDB时，MySQL将在MySQL数据目录下创建一个名为ibdata1的10MB大小的自动扩展数据文件，以及两个名为ib_logfile0和ib_logfile1的5MB大小的日志文件 MyISAM存储引擎MyISAM基于ISAM存储引擎，并对其进行扩展。它是在Web、数据仓储和其他应用环境下最常使用的存储引擎之一。MyISAM拥有较高的插入、查询速度，但不支持事物。MyISAM主要特性有：1、大文件（达到63位文件长度）在支持大文件的文件系统和操作系统上被支持2、当把删除和更新及插入操作混合使用的时候，动态尺寸的行产生更少碎片。这要通过合并相邻被删除的块，以及若下一个块被删除，就扩展到下一块自动完成3、每个MyISAM表最大索引数是64，这可以通过重新编译来改变。每个索引最大的列数是164、最大的键长度是1000字节，这也可以通过编译来改变，对于键长度超过250字节的情况，一个超过1024字节的键将被用上5、BLOB和TEXT列可以被索引6、NULL被允许在索引的列中，这个值占每个键的0~1个字节7、所有数字键值以高字节优先被存储以允许一个更高的索引压缩8、每个MyISAM类型的表都有一个AUTO_INCREMENT的内部列，当INSERT和UPDATE操作的时候该列被更新，同时AUTO_INCREMENT列将被刷新。所以说，MyISAM类型表的AUTO_INCREMENT列更新比InnoDB类型的AUTO_INCREMENT更快9、可以把数据文件和索引文件放在不同目录10、每个字符列可以有不同的字符集11、有VARCHAR的表可以固定或动态记录长度12、VARCHAR和CHAR列可以多达64KB使用MyISAM引擎创建数据库，将产生3个文件。文件的名字以表名字开始，扩展名之处文件类型：frm文件存储表定义、数据文件的扩展名为.MYD（MYData）、索引文件的扩展名时.MYI（MYIndex） MEMORY存储引擎MEMORY存储引擎将表中的数据存储到内存中，未查询和引用其他表数据提供快速访问。MEMORY主要特性有：1、MEMORY表的每个表可以有多达32个索引，每个索引16列，以及500字节的最大键长度2、MEMORY存储引擎执行HASH和BTREE缩影3、可以在一个MEMORY表中有非唯一键值4、MEMORY表使用一个固定的记录长度格式5、MEMORY不支持BLOB或TEXT列6、MEMORY支持AUTO_INCREMENT列和对可包含NULL值的列的索引7、MEMORY表在所由客户端之间共享（就像其他任何非TEMPORARY表）8、MEMORY表内存被存储在内存中，内存是MEMORY表和服务器在查询处理时的空闲中，创建的内部表共享9、当不再需要MEMORY表的内容时，要释放被MEMORY表使用的内存，应该执行DELETE FROM或TRUNCATE TABLE，或者删除整个表（使用DROP TABLE） Archive引擎Archive存储引擎只支持Insert和Select操作，在MySQL5.1之前也不支持索引。Archive引擎会缓存所有的写操作并利用zlib对插入的行进行压缩，所以比MyISAM表的磁盘I/O少。但是每次select查询都需要执行全表扫描。所以Archive表适合日志和数据采集类应用，这类应用做数据分析时往往需要权标骚婊。或者在一些需要更快速的insert操作的场合下也可以使用。 Archive引擎支持行级锁和专用的缓冲区，所以可以实现高并发的插入。在一个查询开始直到返回表中存在的所有行数之前，Archive引擎会组织其他的select执行，以实现一致性读。另外，也实现了批量插入在完成之前对读操作是不可见的。这种机制模仿了事物和MVCC的一些特性，但Archive引擎不是一个事物型的引擎，而是一个针对高速插入和压缩做了优化的简单引擎。 2 面向列的存储引擎MySQL默认是面向行的，每一行的数据时一起存储的，服务器的传也是以行为单位处理的。而在大数据量处理时，可能面向列的方式效率更高。如果不需要整行的数据，面向列的方式可以传输更少的数据。如果每一列都单独村吃醋，那么压缩的效率也会更高。I Infobright是最有名的面向列的存储引擎。在非常大的数据量时（数十TB），该引擎工作良好。Infobright是为数据分析和数据仓库应用设计的。数据高度压缩，按照块进行排序，每个块都对应有一组员数据。在处理查询时，访问元数据可以决定跳过该块进行排序，每个块都对应有一组元数据。在处理查询时，访问元数据可决定跳过该块，甚至可能只需要元数据就可以满足查询的需求。但该引擎不支持索引，不过在这么大的数据量级，即使有索引页很难发挥作用，而且块结构也会一种准索引（quasi-index）。Infobright需要对MySQL服务器做定制，因为一些地方需要修改以适应面向列的存储需要。如果查询无法在存储层使用面向列的模式执行，则需要在服务器层转换成按行处理，这个过程会很慢。Infobright有社区版和商业版两种。 另外一个面向列的存储引擎是Calpont公司的InfiniDB，也有社区版和商业版。InfiniDB可以在一组机器集群间做分布式查询，但目前还没有哦生产环境的应用案例。 3 社区存储引擎如果要列举所有社区提供的引擎可能会有三位数。但是很大部分影响力有限，只有极少数人在使用。在这里举例一些，但都没有在生产环境中应用过，慎用。 ① Aria：之前的名字是Maria，是MySQL创建者计划用来替代MyISAM的一款引擎。MariaDB包含了该引擎，之前计划开发的很多特性因为在MariaDB服务层实现，所以引擎层就取消了。在2013~2014年Aria就是解决了MyISAM的崩溃安全回复问题，当然还有一些特性是MyISAM不具备的，例如 数据的缓存（MyISAM只能缓存索引）。 ② Groonga：这是一款全文索引引擎，号称可以提供准确而高效的全文索引。 ③ OQGraph：该引擎由uOpen Query研发，支持图操作（例如查找两点之间最短的路径），用SQL很难实现该类操作。 ④ Q4M：该引擎在MySQL内部实现了队列操作，这也是SQL很难实现的操作。 ⑤ SphinxSE：该引擎为Sphinx全文索引搜索服务提供了SQL接口。 ⑥ Spider：该引擎可以将数据切分成不同的分区，比较高效透明的实现了分片（shard），并且可以针对分片执行并行查询（可以是分布式的分片）。 ⑦ VPForMySQL：改引擎支持垂直分区，通过一系列的代理存储引擎是新。垂直分区指的是可以将表分成不同列的这，并且单独存储。但对于查询来说，看到的还是一张表。改引擎和Spider的作者是同一人。 转换表的引擎下面接受三种Mysql 数据库将表的存储引擎转换成另外一种引擎。每种方法都有优缺点。 ALTER TABLE将表的一个引擎修改为另个引擎最简单的办法就是是用alter table 语句，需要执行很长时间。Mysql会按行讲源数据复制到另一新表当中，在复制期间可能会消耗系统所有的I/O能力，同时原表会加上锁。所以在繁忙的表上执行此操作要下心。如果转换表的存储引擎将会丢失和原引擎相关的所有特性。如，将一张InnoDB表转换为MyISAM，然后转换InnoDB，原InnoDB上的所有外键将会丢失。 导入和导出为了更好的控制转换过程，可是使用mysqldump 工具将数据导入文件中，然后修改文件中的create table 语句中的存储引擎选项，mysqldump 工具默认会在create table 中加上drop 语句。 创建和查询第三种装换技术综合了第一种的高效和第二种方法的中的安全，不需要导出真个表的数据。而是先创建一个新的存储引擎的表。然后利用 Insert 。。。。select 语句来导出， Mysql常见索引有：主键索引、唯一索引、普通索引、全文索引、组合索引 PRIMARY KEY（主键索引） ALTER TABLE table_name ADD PRIMARY KEY ( col ) UNIQUE(唯一索引) ALTER TABLE table_name ADD UNIQUE (col) INDEX(普通索引) ALTER TABLE table_name ADD INDEX index_name (col) FULLTEXT(全文索引) ALTER TABLE table_name ADD FULLTEXT ( col )组合索引 ALTER TABLE table_name ADD INDEX index_name (col1, col2, col3 ) Mysql各种索引区别：普通索引：最基本的索引，没有任何限制唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。主键索引：它 是一种特殊的唯一索引，不允许有空值。全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。创建复合索引时应该将最常用（频率）作限制条件的列放在最左边，依次递减。 where 字段从左往右顺序，数据量最少的字段放在最左边，因为这样查询次数会最少","categories":[],"tags":[]},{"title":"git 自己的总结","slug":"git 自己的总结","date":"2019-11-14T12:36:09.306Z","updated":"2019-10-31T00:09:04.665Z","comments":true,"path":"2019/11/14/git 自己的总结/","link":"","permalink":"http://yoursite.com/2019/11/14/git%20%E8%87%AA%E5%B7%B1%E7%9A%84%E6%80%BB%E7%BB%93/","excerpt":"","text":"1先在码云官网创建自己的项目略 2在桌面 拉去远端码云项目到本地:1git clone https://gitee.com/edward_h/OnlineEducation.git 3新建分支,并关联远程分支方便以后上传项目文件,注意 有些个性化配置需要上传,比如vue项目的 node_modules1git checkout -b dev # 创建分支并切换到该分支 切换分支 （dev是分支名)1git checkout dev # 切换分支 （dev是分支名) #这步不用建立分支了 查看分支1git branch 4.创建远程分支连接(此处为master分支)1git branch --set-upstream-to=origin/master 如果提示错误,则说明远程没有该分支,则需要创建该远程分支 创建完毕后,再执行4操作,关联远程分支 5.执行添加项目操作,这两步操作必须同时存在123git add -A 文件名 :添加修改文件 git commit -m \"提交的信息\" 将文件存进暂存区 6.此处拉下文件如果提示冲突,则解决完冲突,再次执行第五步操作,因为修改了冲突的代码,所以重新将代码添加到暂存区1git pull # 下拉最新文件 1234git add 文件名 :将文件存进暂存区 git add -A 提交所有修改git commit -m \"提交的信息\" 7.直到不显示冲突则说明解决完冲突了,再提交1git push # 上传文件 不提示错误,则上传完毕 8.可以查看上一次的修改操作是什么1git status 备用知识点 建项目着 如果想让其他人通过输入账号密码才能上传文件 则输入如下命令 (这些命令用不到)插入如下代码使项目中的组员每次修改数据都要输入账号和密码： git config –global user.name [username] git config –global user.password [userpassword] PS：想要保存密码，则需要插入如下代码： git config –global credential.helper store。 ###gitee推送到远程仓库时提示错误 remote: Incorrect username or password ( access token ) fatal: Authentication failed for ‘https://gitee.com/***/***.git/&#39; 解决办法：清除本地的gitee用户名和密码 git config –system –unset credential.helper 再执行推送，重新输入用户名和密码。","categories":[],"tags":[]},{"title":"day01","slug":"day01","date":"2019-11-14T12:36:09.304Z","updated":"2019-10-23T23:52:11.530Z","comments":true,"path":"2019/11/14/day01/","link":"","permalink":"http://yoursite.com/2019/11/14/day01/","excerpt":"","text":"php7和php5之间的区别：1、性能提升：PHP7比PHP5.0性能提升了两倍。 2、全面一致的64位支持。 3、以前的许多致命错误，现在改成抛出异常。 4、PHP 7.0比PHP5.0移除了一些老的不在支持的SAPI（服务器端应用编程端口）和扩展。 5、PHP 7.0比PHP5.0新增了空接合操作符。 6、PHP 7.0比PHP5.0新增加了结合比较运算符。 7、PHP 7.0比PHP5.0新增加了函数的返回类型声明。 8、PHP 7.0比PHP5.0新增加了标量类型声明。 9、PHP 7.0比PHP5.0新增加匿名类。 为什么 PHP7 比 PHP5 性能提升了？ 1、变量存储字节减小，减少内存占用，提升变量操作速度 2、改善数组结构，数组元素和hash映射表被分配在同一块内存里，降低了内存占用、提升了 cpu 缓存命中率 3、改进了函数的调用机制，通过优化参数传递的环节，减少了一些指令，提高执行效率 路径 lamp是什么？LAMP是一个开源 Web开发平台，它使用Linux作为操作系统，Apache作为Web服务器，MySQL作为关系数据库管理系统，PHP作为面向对象的脚本语言。（有时使用Perl或Python代替PHP。） 因为平台具有四个层 ，LAMP有时被称为LAMP堆栈。堆栈可以构建在不同的操作系统上。据说使用这些工具与Windows操作系统而不是Linux的开发人员正在使用WAMP ; 使用Macintosh系统，MAMP; 和Solaris系统，SAMP。 AMP背后的理念的关键是，这个术语最初由Michael Kunze在1998年德国杂志c’t中创造，将这些物品一起使用。尽管实际上并不是为了协同工作而设计的，但这些开源软件替代品可以随时随地获得，因为LAMP堆栈中的每个组件都是免费或开源软件（FOSS）的一个例子。 LAMP已成为事实上的发展标准。今天，构成LAMP堆栈的产品默认包含在几乎所有Linux发行版中，并且它们共同构成了一个功能强大的Web应用程序平台。最初的LAMP缩写产生了许多其他相关的首字母缩略词，这些首字母缩略词充分利用了原始技术组合的主要焦点，以提供功能丰富的网站。其中一些相关的Web堆栈包括LAPP，MAMP和BAMP。 有些人认为LAMP是：Linux，Apache，MySQL和Perl / PHP / Python。换句话说，解决方案堆栈中的脚本语言可以是Perl，PHP或Python。PHP是最流行的脚本语言，因此，在实践中，LAMP通常用于描述带有PHP语义的解决方案堆栈，主要思想是相同的 – LAMP是一个广泛使用的开源解决方案堆栈，被认为相对容易使用。 Apache和nginx的区别Nginx 轻量级，采用 C 进行编写，同样的 web 服务，会占用更少的内存及资源 抗并发，nginx 以 epoll and kqueue 作为开发模型，处理请求是异步非阻塞的，负载能力比 apache 高很多，而 apache 则是阻塞型的。在高并发下 nginx 能保持低资源低消耗高性能 ，而 apache 在 PHP 处理慢或者前端压力很大的情况下，很容易出现进程数飙升，从而拒绝服务的现象。 nginx 处理静态文件好，静态处理性能比 apache 高三倍以上 nginx 的设计高度模块化，编写模块相对简单 nginx 配置简洁，正则配置让很多事情变得简单，而且改完配置能使用 -t 测试配置有没有问题，apache 配置复杂 ，重启的时候发现配置出错了，会很崩溃 nginx 作为负载均衡服务器，支持 7 层负载均衡 nginx 本身就是一个反向代理服务器，而且可以作为非常优秀的邮件代理服务器 启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动，还能够不间断服务的情况下进行软件版本的升级 社区活跃，各种高性能模块出品迅速 Apache apache 的 rewrite 比 nginx 强大，在 rewrite 频繁的情况下，用 apache apache 发展到现在，模块超多，基本想到的都可以找到 apache 更为成熟，少 bug ，nginx 的 bug 相对较多 apache 超稳定 apache 对 PHP 支持比较简单，nginx 需要配合其他后端用 apache 在处理动态请求有优势，nginx 在这方面是鸡肋，一般动态请求要 apache 去做，nginx 适合静态和反向。 apache 仍然是目前的主流，拥有丰富的特性，成熟的技术和开发社区 总结两者最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，而 nginx 是异步的，多个连接（万级别）可以对应一个进程 一般来说，需要性能的 web 服务，用 nginx 。如果不需要性能只求稳定，更考虑 apache ，后者的各种功能模块实现得比前者，例如 ssl 的模块就比前者好，可配置项多。epoll(freebsd 上是 kqueue ) 网络 IO 模型是 nginx 处理性能高的根本理由，但并不是所有的情况下都是 epoll 大获全胜的，如果本身提供静态服务的就只有寥寥几个文件，apache 的 select 模型或许比 epoll 更高性能。当然，这只是根据网络 IO 模型的原理作的一个假设，真正的应用还是需要实测了再说的。 更为通用的方案是，前端 nginx 抗并发，后端 apache 集群，配合起来会更好。 nextcloud的盈利方式一，卖网盘容量网盘最终目的当然是赚钱。首先要有用户才能有客户，一个产品，不论多么完美、多么强大，没有用户，就相当于什么也没有。网盘空间大战（主要集中于个人网盘）是必然的，目的在于拉拢用户，有了一定的用户基数，才能开始下一步的计划。而企业网盘级网盘则采用免费一部分网盘+灵活增值扩容的模式。 二，卖速度网盘中的东西，一定会被下载下来的，当然也有例外。下载速度就尤为重要了，纵然你有1000Mbps的带宽，网盘方面实施限速，下载速度依然会很慢，这样速度就可以作为商品来出售了。 三，卖功能用户最终愿不愿意付费，还是取决于网盘的功能，看能否满足用户的发展需要。如果能满足，解决了用户的基本需求，那么即体现了网盘使用价值的意义。 四，卖客户端客户端不是卖给客户的，而是卖给广告主的。目前大多数网盘为何都能覆盖全平台（Web、PC、Android、iPhone、iPad、Mac），其一在于给更多的用户提供便利，其二在于获取更多的用户资源，其三有了一定的用户群体之后，可以做点别的什么，比如，广告之类的。（当然个人网盘可能会先有，企业网盘就不得而知了） 五，增值业务这里增值业务主要指外链分享或单个文件上传大小限制，主要面向那些喜欢在网络中分享资源的人，像小型站长，因为网站的主机空间太贵，只能把大一点的图片或电子资源存放在其它位置。当需要分享更大的空间时，即可请用户付费。 六，定制化因大多数的网盘都是针对成千上万用户的，那么每个用户所用到的功能基本上都是一致的。但有的客户因资金或实力过硬，需要一套印有自己企业印记或者说与众不同的网盘功能，此时可以考虑私有云定制。目前市面上仅仅只有少数几家可以提供私有云定制服务，能够无缝集成OA、CRM、ERP等系统。 七，总的来说，从个人角度看，网盘大多现在都还无法盈利，基本上都是亏本。那为什么还有这么多人做网盘？因为大家都免费，所以你也必须免费，既然免费甚至亏本那还要做？其实现在是在圈地，积累人气，人气够了，然后再盈利。 vue.js发送cookie","categories":[],"tags":[]},{"title":"Archive引擎","slug":"Archive引擎","date":"2019-11-14T12:36:09.302Z","updated":"2019-10-31T00:19:39.224Z","comments":true,"path":"2019/11/14/Archive引擎/","link":"","permalink":"http://yoursite.com/2019/11/14/Archive%E5%BC%95%E6%93%8E/","excerpt":"","text":"1 Archive引擎Archive存储引擎只支持Insert和Select操作，在MySQL5.1之前也不支持索引。Archive引擎会缓存所有的写操作并利用zlib对插入的行进行压缩，所以比MyISAM表的磁盘I/O少。但是每次select查询都需要执行全表扫描。所以Archive表适合日志和数据采集类应用，这类应用做数据分析时往往需要权标骚婊。或者在一些需要更快速的insert操作的场合下也可以使用。 Archive引擎支持行级锁和专用的缓冲区，所以可以实现高并发的插入。在一个查询开始直到返回表中存在的所有行数之前，Archive引擎会组织其他的select执行，以实现一致性读。另外，也实现了批量插入在完成之前对读操作是不可见的。这种机制模仿了事物和MVCC的一些特性，但Archive引擎不是一个事物型的引擎，而是一个针对高速插入和压缩做了优化的简单引擎。 2 Blackhole引擎Blackhole引擎没有实现任何的存储机制，它会丢弃所有插入的数据，不做任何保存。但是服务器会记录Blackhole表的日志，所以可以用于赋值数据库到备库，或者只是简单的记录到日志。这种特殊的存储引擎可以在一些特殊的复制架构和日志审核时发挥作用。但这种应用方式会产生很多问题，因此不推荐。 3 CSV引擎CSV引擎可以将普通的CSV文件（逗号分割值的文件）作为MySQL的表来处理，但这种表不支持索引。CSV引擎可以在数据库运行时拷入或拷出文件。可以将Excel等调子表格软件中的数据存储为CSV文件，然后复制到MySQL数据目录下，就能在MySQL中打开使用。同样，如果将数据写入到一个CSV引擎表，其他的外部程序也能立即从表的数据文件中读取CSV格式的数据。因此CSV引擎可以作为一种数据交换的机制，非常有用。 4 Federated引擎Federated引擎是访问其他MySQL服务器的一个代理，它会创建一个到远程MySQL服务器的客户端连接，并将查询传输到远程服务器执行，然后提取或者发送需要的数据。最初设计该存储引擎是为了和企业级数据库如Microsoft SQL Server和Oracle的类似特性竞争的，可以说更多的是一种市场行为。尽管该引擎看起来提供了一种很好的跨服务器的灵活性，但也经常带来问题，因此默认是禁用的。MariaDB使用了它的一个后续改进版本，叫做FederatedX。 5 Memory引擎如果需要快速的访问数据，并且这些数据不会被修改，重启以后丢失也没有关系，那么使用Memory表（以前也叫HEAP表）是非常有用的。Memory表至少比MyISAM表要快一个数量级，因此所有的数据都保存在内存中，不需要进行磁盘I/O。Memory表的结构在重启以后还会保留，但数据会丢失。 Memroy的应用场景： ① 用于查找（lookup）或者映射表，例如将邮编和城市名映射的表 ② 用于缓存周期性聚合数据（periodically aggregated data）的结果。 ③ 用于保存数据分析中产生的中间数据Memory表支持Hash索引，因此查找操作非常快。虽然Memory表的速度非常快，但还是无法取代传统的基于磁盘的表。Memroy表时表级锁，因此并发写入的性能较低。它不支持BLOB或TEXT类型的列，并且每行的长度是固定的所以即使制定了VARCHAR列，实际存储时也会转换成CHAR，这可能导致部分内存的浪费（其中一些限制在Percona版本中已解决）。 如果MySQL在执行查询的过程中需要使用临时表来保存中间结果，内部使用的临时表就是Memory表。如果中间结果太大超出了Memory表的限制，或者含有BLOB或TEXT字段 ，则临时表会转换成MyISAM表。 注： 人们经常混淆Memory表和临时表。临时表是指使用create temporary table 语句创建的表，它可以使用任何存储引擎，因此和Memory表不是一回事。临时表只在单个连接中可见，当连接断开时，临时表将删除。 6 Merge引擎Merge引擎是MyISAM引擎的一个变种。Merge表时由多个MyISAM表合并而来的虚拟表。如果将MySQL用于日志或者数据仓库类应用，该引擎可以发挥作用。但是引入分区功能后，该引擎已经被放弃。 7 NDB集群引擎2003年时MySQL AB公司从索尼爱立信公司收购了NDB数据库，然后开发了NDB集群存储引擎，作为SQL和NDB原生协议之间的接口。MySQL服务器、NDB集群存储引擎，以及分布式的、share-nothing的容灾的、高可用的NDB数据库的组合，被从未MySQL集群（MySQL Cluster）。后面应该会单独写一篇文章来详细讲解MySQL集群 1 OLTP类引擎Percona的XtraDB存储引擎是基于InnoDB引擎的一个改进版本，已经包含在Percona Server和MariaDB中，它的改进点主要集中在性能、可测量性和操作灵活性方面。XtraDB可以作为InnoDB的一个完全的替代产品，甚至可以兼容的读写InnoDB的数据文件，并支持InnoDB的所有查询。 另外还有一些和InnoDB非常类似的OLTP类存储引擎，比如都支持ACID事物和MVCC。其中一个就是PBXT，由PaulMcCullagh和Primebase GMBH开发。它支持引擎级别的赋值、外键约束，并且一一种比较复杂的架构对固态存储（SSD）提供了适当的支持，还对较大的值类型如BLOB也做了优化。PBXT是一款社区支持的存储引擎，MariaDB包含了该引擎。 TokuDB引擎使用了一种新的叫做分型树（Fractal Trees）的索引数据结构。该结构是缓存无关的，因此即使其大小超过内存，性能也不会下降，也就没有内存生命周期和碎片的问题。TokuDB是一种大数据（Big Data）存储引擎，因为其拥有很高的压缩比，可以在很大的数据量上创建大量索引。 RethinkDB最初是为固态存储（SSD）而设计的，然而随着时间的推移，目前看起来和最初的目标有一定的差距。该引擎比较特别的地方在于采用了一种智能追加的写时复制B树（append-only copyon-write B-Tree）作为索引的数据结构。 在Sun收购了MySQL AB以后，Falcon存储引擎曾经作为下一代存储引擎被寄予期望，但现在该项目已经在很久以前就被取消了。 2 面向列的存储引擎MySQL默认是面向行的，每一行的数据时一起存储的，服务器的传也是以行为单位处理的。而在大数据量处理时，可能面向列的方式效率更高。如果不需要整行的数据，面向列的方式可以传输更少的数据。如果每一列都单独村吃醋，那么压缩的效率也会更高。I Infobright是最有名的面向列的存储引擎。在非常大的数据量时（数十TB），该引擎工作良好。Infobright是为数据分析和数据仓库应用设计的。数据高度压缩，按照块进行排序，每个块都对应有一组员数据。在处理查询时，访问元数据可以决定跳过该块进行排序，每个块都对应有一组元数据。在处理查询时，访问元数据可决定跳过该块，甚至可能只需要元数据就可以满足查询的需求。但该引擎不支持索引，不过在这么大的数据量级，即使有索引页很难发挥作用，而且块结构也会一种准索引（quasi-index）。Infobright需要对MySQL服务器做定制，因为一些地方需要修改以适应面向列的存储需要。如果查询无法在存储层使用面向列的模式执行，则需要在服务器层转换成按行处理，这个过程会很慢。Infobright有社区版和商业版两种。 另外一个面向列的存储引擎是Calpont公司的InfiniDB，也有社区版和商业版。InfiniDB可以在一组机器集群间做分布式查询，但目前还没有哦生产环境的应用案例。 3 社区存储引擎如果要列举所有社区提供的引擎可能会有三位数。但是很大部分影响力有限，只有极少数人在使用。在这里举例一些，但都没有在生产环境中应用过，慎用。 ① Aria：之前的名字是Maria，是MySQL创建者计划用来替代MyISAM的一款引擎。MariaDB包含了该引擎，之前计划开发的很多特性因为在MariaDB服务层实现，所以引擎层就取消了。在2013~2014年Aria就是解决了MyISAM的崩溃安全回复问题，当然还有一些特性是MyISAM不具备的，例如 数据的缓存（MyISAM只能缓存索引）。 ② Groonga：这是一款全文索引引擎，号称可以提供准确而高效的全文索引。 ③ OQGraph：该引擎由uOpen Query研发，支持图操作（例如查找两点之间最短的路径），用SQL很难实现该类操作。 ④ Q4M：该引擎在MySQL内部实现了队列操作，这也是SQL很难实现的操作。 ⑤ SphinxSE：该引擎为Sphinx全文索引搜索服务提供了SQL接口。 ⑥ Spider：该引擎可以将数据切分成不同的分区，比较高效透明的实现了分片（shard），并且可以针对分片执行并行查询（可以是分布式的分片）。 ⑦ VPForMySQL：改引擎支持垂直分区，通过一系列的代理存储引擎是新。垂直分区指的是可以将表分成不同列的这，并且单独存储。但对于查询来说，看到的还是一张表。改引擎和Spider的作者是同一人。 转换表的引擎下面接受三种Mysql 数据库将表的存储引擎转换成另外一种引擎。每种方法都有优缺点。 ALTER TABLE将表的一个引擎修改为另个引擎最简单的办法就是是用alter table 语句，下面是修改user 表的储存引擎； mysql&gt; alter table user engine=InnoDB;Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show table status like ‘user’ \\G;1234上述语法可以适合任何储存引擎。但是有一个问题：需要执行很长时间。Mysql会按行讲源数据复制到另一新表当中，在复制期间可能会消耗系统所有的I/O能力，同时原表会加上锁。所以在繁忙的表上执行此操作要下心。如果转换表的存储引擎将会丢失和原引擎相关的所有特性。如，将一张InnoDB表转换为MyISAM，然后转换InnoDB，原InnoDB上的所有外键将会丢失。 导入和导出为了更好的控制转换过程，可是使用mysqldump 工具将数据导入文件中，然后修改文件中的create table 语句中的存储引擎选项，mysqldump 工具默认会在create table 中加上drop 语句。 创建和查询第三种装换技术综合了第一种的高效和第二种方法的中的安全，不需要导出真个表的数据。而是先创建一个新的存储引擎的表。然后利用 Insert 。。。。select 语句来导出， mysql&gt; create table myInnoDB like user;Query OK, 0 rows affected (0.04 sec) mysql&gt; alter table myInnoDB engine=innodb;Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0 mysql&gt; insert into myInnoDB select * from user;","categories":[],"tags":[]},{"title":"Python","slug":"Python","date":"2019-11-14T08:51:23.256Z","updated":"2019-11-14T11:51:26.923Z","comments":true,"path":"2019/11/14/Python/","link":"","permalink":"http://yoursite.com/2019/11/14/Python/","excerpt":"","text":"冒泡排序：它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。 时间复杂度：O(n²)空间复杂度：O(1)稳定性：稳定 12345678910def bubble_sort(blist): count = len(blist) for i in range(0, count): for j in range(i + 1, count): if blist[i] &gt; blist[j]: blist[i], blist[j] = blist[j], blist[i] return blistblist = bubble_sort([4,5,6,7,3,2,6,9,8])print(blist) 快排：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 时间复杂度：O(nlog₂n)空间复杂度：O(nlog₂n)稳定性：不稳定 12345678910111213141516171819#O(n*(log n))def quicksort(array): if len(array) &lt; 2: return array else: pivot = array[0] #找到一个基准值 #遍历整个列表，将小于这个基准值的元素放到一个子列表中 less = [i for i in array[1:] if i &lt; pivot] #遍历整个列表，将大于这个基准值的元素放到一个子列表中 greater = [i for i in array[1:] if i&gt;pivot] #首先，明确我们对元素为0个/1个的列表无需要排序 #使用函数递归 #目标：让我们在一个基准值的一侧变为有序，然后依次返回，让我们的每个基准值的两侧都变得有序 return quicksort(less)+[pivot]+quicksort(greater)#这是一些测试样例print(quicksort([2,5,3,9,7,11]))print(quicksort([152,134,38796,7438415,1,2272,34345,24,127])) 单例：保证一个类只有一个实例，并提供一个访问它的全局访问点 优点：对唯一实例的受控访问，相当于全局变量，但是又可以防止变量被篡改 通过new方法，将类的实例在创建的时候绑定到类属性_inst上。如果cls._inst为None，说明类还未实例化，实例化并将实例绑定到cls._inst，以后每次实例化的时候都返回第一次实例化创建的实例。注意从Singleton派生子类的时候，不要重载new。因为类每一次实例化后产生的过程都是通过new来控制的，所以通过重载new方法，我们 可以很简单的实现单例模式。 123456789101112class Single(object): _instance = None def __new__(cls, *args, **kw): if cls._instance is None: cls._instance = object.__new__(cls, *args, **kw) return cls._instance def __init__(self): passsingle1 = Single()single2 = Single()print(id(single1) , id(single2)) _ _ new _ _方法:使用类名()创建对象时，Python的解释器首先会调用new方法为对象分配空间._ _ new _ _是一个有object基类提供的内置的静态方法，主要作用有两个： 在内存中为对象分配空间 返回对象的引用Python的解释器获得对象的引用后，将引用作为第一个参数，传递给init方法。重写new方法的代码非常固定。重写new方法一定要return super.new(cls)，否则Python的解释器得不到分配了空间的对象引用，就不会调用对象的初始化方法注意：new是一个静态方法，在调用时需要主动传递参数clsnew至少要有一个参数cls，代表要实例化的类，此参数在实例化时由Python解释器自动提供 递归斐波那契数列：1234567891011def fun(i): if i == 0: return 0 elif i == 1: return 1 else: return fun(i-2) + fei(i-1)if __name__ == '__main__': for i in range(10): print(fun(i),end=\" \") 递归遍历目录：123456789101112131415import osdef fun(p): for i in os.listdir(p): i = os.path.join(p,i) if os.path.isdir(i): return fun(i) if os.path.splitext(i)[1] == '.txt': print i else: return fun(i)path = unicode(r'F:\\My Study\\linux学习笔记\\test','utf-8') #需要遍历的目录f(path) 青蛙跳台阶参考链接：https://blog.csdn.net/Bryce_Liu/article/details/90639516 闭包：闭包的定义： 在一个外函数中定义了一个内函数 内函数里运用了外函数的临时变量 并且外函数的返回值是内函数的引用。这样就构成了一个闭包。 12345678def outer(): a = 10 def inner(): b = a + 10 print(\"b =\",b) return innerouter()() 装饰器测试程序运行时间： 使用装饰器测试（1000以内的三个数，相加等于1000的情况，有多少组）这个案例的运行时间 1234567891011121314151617181920import timedef time_(fun): def inner(): s_time = time.time() #获取程序运行的开始时间 fun() #运行程序 e_time = time.time() #获取程序运行的结束时间 return e_time-s_time return inner @time_def func(): for a in range(1001): for b in range(1001): c = 1000 - a - b if a ** 2 + b ** 2 == c ** 2: print(\"a = %d , b = %d , c = %d\" % (a,b,c) if __name__ == '__main__': print(func()) python实现树的遍历：参考链接：https://blog.csdn.net/bone_ace/article/details/46718683 广度遍历：层次遍历深度遍历：先、中、后序遍历 层次遍历：一层一层的遍历先序遍历：依据 根–左–右 的顺序遍历中序遍历：依据 左–根–右 的顺序遍历后序遍历：依据 左–右–根 的顺序遍历 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class Node(object): '''树的节点''' def __init__(self, item): self.elem = item self.lchild = None #左孩子 self.rchild = None #右孩子class Tree(object): '''二叉树''' def __init__(self): self.root = None # 根节点 def add(self,item): '''添加的方法''' node = Node(item) # 先构造一个节点 if self.root is None: # 如果是空树 直接添加元素 self.root = node return queue = [self.root] # 一个队列 用来存放的就是要遍历和处理的元素 while queue: #队列只要不为空 就始终能拿出节点进行判断 # 先从队列中取出一个节点 cur_node = queue.pop(0) # 看当前这个节点左边的孩子是否为空 如果是空 直接挂节点 if cur_node.lchild is None: cur_node.lchild = node return else: # 不为空则认定左孩子存在 追加到队列 queue.append(cur_node.lchild) # 查看节点右孩子 与左孩子同理 if cur_node.rchild is None: cur_node.rchild = node return else: queue.append(cur_node.rchild) def breadth_trvael(self): '''层次遍历''' if self.root == None: return queue = [self.root] while queue: # 只要队列不为空就一直取元素 cur_node = queue.pop(0) print(cur_node.elem,end=' ') # 如果左孩子存在 添加到队列中 if cur_node.lchild: queue.append(cur_node.lchild) # 右孩子同理 if cur_node.rchild: queue.append(cur_node.rchild) def preorder(self, node): #传一个根节点 '''先序遍历''' if node == None: #递归的终结条件 return print(node.elem,end=' ') #先打印根 self.preorder(node.lchild) #处理左半部分 self.preorder(node.rchild) #处理右半部分 def inorder(self, node): '''中序遍历''' if node == None: return self.inorder(node.lchild) # 先处理左部分 print(node.elem, end=' ') #输出根 self.inorder(node.rchild) #再处理右半部分 def postorder(self, node): '''后序遍历''' if node == None: return self.postorder(node.lchild) # 先处理左部分 self.postorder(node.rchild) # 然后处理右半部分 print(node.elem, end=' ') # 最后输出根if __name__ == '__main__': tree = Tree() # 添加元素 tree.add(0) tree.add(1) tree.add(2) tree.add(3) tree.add(4) tree.add(5) tree.add(6) tree.add(7) tree.add(8) tree.add(9) print('层次遍历：') tree.breadth_trvael() print('\\n') print('先序遍历：') tree.preorder(tree.root) print('\\n') print('中序遍历：') tree.inorder(tree.root) print('\\n') print('后序遍历：') tree.postorder(tree.root) print('\\n') 链表：参考链接：https://www.jianshu.com/p/9f2aca048c84https://www.jb51.net/article/133798.htm 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112class Node(object): '''节点类''' def __init__(self,elem): self.elem = elem self.next = Noneclass SingleLinkList(object): '''单链表''' def __init__(self, node=None): self.head = node def is_empty(self): '''判断链表是否为空''' return self.head == None #如果头节点为空 列表就为空 def length(self): '''链表长度''' cur = self.head #cur游标 用来移动遍历节点 count = 0 #记录数量 while cur != None: count += 1 cur = cur.next #移动游标 return count def travel(self): '''遍历整个链表''' cur = self.head #代表第一个节点 while cur != None: print(cur.elem) cur = cur.next def add(self, item): '''在链表头部添加元素，头插法''' node = Node(item) node.next = self.head # 新元素的下一个节点指向链表第一个元素 self.head = node #头节点指向新元素 def append(self, item): '''向链表的尾部添加元素,尾插法''' node = Node(item) if self.is_empty(): #如果链表为空 self.head = node #头节点指向添加的元素 else: #不为空 cur = self.head # 游标 while cur.next != None: # 游标下一个位置不为空开始进入循环 为空则不进入循环 cur = cur.next # 游标移动 cur.next = node #当游标下一位置为空时添加元素 def insert(self, pos, item): # 传入一个插入位置pos 一个插入元素item '''指定位置添加元素''' # 如果添加位置在头部 直接使用头插入方法 if pos &lt;= 0 : self.add(item) elif pos &gt; (self.length()-1): #插入位置超出列表范围 使用尾插法 self.append(item) else: cur = self.head count = 0 while count &lt; (pos-1): count += 1 cur = cur.next # 当循环退出后cur指向 pos-1位置 node = Node(item) node.next = cur.next cur.next = node def remove(self, item): '''删除元素,根据具体的数据删除''' cur = self.head pre = None # 前一位置 while cur != None: if cur.elem == item: # 先判断子节点是否为头节点 if cur == self.head: self.head = cur.next # 直接改变头指针指向的地址 else: pre.next = cur.next # 如果删除尾部 cur.next刚好指向none break else: pre = cur cur = cur.next def search(self, item): '''查找节点是否存在''' cur = self.head while cur != None: # 列表不为空时 if cur.elem == item: return True else: cur = cur.next return Falseif __name__ == '__main__': ll = SingleLinkList() print('是否为空:',ll.is_empty()) print('链表长度：',ll.length()) # 添加元素 ll.append(2) #尾部添加 ll.append(6) ll.add(8) #头部添加 ll.append(1) ll.insert(3,9) print('开始遍历链表元素：') ll.travel() # 删除一个元素 ll.remove(6) print('删除后遍历：') ll.travel() print('是否为空:',ll.is_empty()) print('链表长度：',ll.length()) print('元素是否存在：',ll.search(10)) print('元素是否存在：',ll.search(9)) 栈与队列：参考链接：https://blog.csdn.net/yushupan/article/details/82312819https://www.jianshu.com/p/1327cc0de255 栈和队列是两种基本的数据结构，同为容器类型。 两者根本的区别在于： 栈stack:后进先出 队列queue:先进先出 栈的构造：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647\"\"\"Stack() 创建一个新的空栈push(item) 添加一个新的元素item到栈顶pop() 弹出栈顶元素peek() 返回栈顶元素is_empty() 判断栈是否为空size() 返回栈的元素个数\"\"\"class Stack(object): \"\"\"栈类\"\"\" def __init__(self): \"\"\"创建一个空栈\"\"\" self.stack = [] def push(self,item): \"\"\"添加一个新的元素到栈顶\"\"\" self.stack.append(item) def pop(self): \"\"\"弹出栈顶元素\"\"\" return self.stack.pop() def peek(self): \"\"\"返回栈顶元素\"\"\" return self.stack[-1] def is_empty(self): \"\"\"判断栈是否为空\"\"\" return self.stack == [] def size(self): \"\"\"返回栈元素的个数\"\"\" return len(self.stack)if __name__ == '__main__': s = Stack() print(s.is_empty()) print(s.size()) print(\"*\"*10) s.push(2) s.push(6) s.push(3) print(s.stack) print(s.pop()) print(s.size()) print(s.pop()) print(s.size()) print(s.pop()) print(s.size()) print(\"*\"*10) print(s.is_empty()) print(s.size()) 队列的构造：123456789101112131415161718192021222324252627282930313233343536373839404142\"\"\"Queue() 创建一个空的队列enqueue(item) 往队列中添加一个item元素dequeue() 从队列头部删除一个元素is_empty() 判断一个队列是否为空size() 返回队列的大小\"\"\"class Queue(object): \"\"\"队列\"\"\" def __init__(self): \"\"\"创建一个空队列\"\"\" self.queue = [] def enqueue(self,item): \"\"\"往队列中添加一个元素\"\"\" self.queue.append(item) def dequeue(self): \"\"\"从队列头部删除一个元素\"\"\" return self.queue.pop(0) def ie_empty(self): \"\"\"判断一个队列是否为空\"\"\" return self.queue == [] def size(self): \"\"\"返回队列大小\"\"\" return len(self.queue)if __name__ == '__main__': q = Queue() print(q.ie_empty()) print(q.size()) print(\"*\"*10) q.enqueue(5) q.enqueue(6) q.enqueue(7) print(q.dequeue()) print(q.size()) print(\"*\"*10) print(q.dequeue()) print(q.size()) print(\"*\"*10) print(q.dequeue()) print(q.ie_empty()) print(q.size())","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"},{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-10-21T03:44:14.147Z","updated":"2019-10-21T03:44:14.148Z","comments":true,"path":"2019/10/21/hello-world/","link":"","permalink":"http://yoursite.com/2019/10/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}